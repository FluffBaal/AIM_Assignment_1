# Phase 4: Benchmark Core Pipeline

**Date**: 2025-06-26  
**Branch**: `phase-4/benchmark-core`

## Summary
Implemented the core benchmark pipeline with NDJSON streaming, prompt versioning, and robust error handling.

## Changes

### Backend
- Created `backend/models.py` with benchmark data models:
  - `BenchRequest`: Benchmark configuration
  - `EvalResult`: Evaluation outcomes
  - Event models: `AnswerEvent`, `EvalEvent`, `ToolEvent`, `SummaryEvent`
  
- Implemented `backend/benchmark.py`:
  - `run_benchmark()`: Async generator pipeline
  - `compute_prompt_hash()`: SHA256 prompt versioning
  - `fix_json_with_retry()`: JSON recovery with LLM fallback
  - `evaluate_response()`: Pluggable evaluation logic

- Added `/api/benchmark` endpoint:
  - Streams NDJSON events in real-time
  - Content-Type: `application/x-ndjson`

### Testing
- Comprehensive unit tests in `test_benchmark.py`:
  - Hash computation consistency
  - JSON fixing with various malformations
  - Event ordering verification
  - Error handling scenarios
  - Tool call detection

### Fixtures
- Sample NDJSON in `tests/fixtures/sample_run.ndjson`
- Demonstrates all event types with realistic data

### Documentation
- ADR 0004: Benchmark core architecture decisions

## Technical Decisions
1. **NDJSON Format**: Line-delimited JSON for streaming compatibility
2. **SHA256 Hashing**: Consistent prompt versioning across runs
3. **Async Generators**: Memory-efficient event streaming
4. **Graceful Degradation**: Continue on errors, report in summary

## Next Phase
Phase 5 will build the frontend components to visualize benchmark runs and results.